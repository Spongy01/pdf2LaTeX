{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj-_58r3lHvI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "class LatexIndexer:\n",
        "    def __init__(self, similarity_threshold=0.8, max_pages=10):\n",
        "        self.index_entries = {}\n",
        "        self.modified_content = []\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.max_pages = max_pages\n",
        "        self.debug = True\n",
        "\n",
        "    def parse_index_list(self, index_text):\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "        lines = [line for line in lines if not line.startswith('\\\\begin') and not line.startswith('\\\\end')]\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'(.*?),\\s*((?:\\d+(?:,\\s*\\d+)*))\\\\?$', line)\n",
        "            if match:\n",
        "                term, pages = match.groups()\n",
        "                term = term.strip()\n",
        "                pages = [p.strip() for p in pages.split(',')]\n",
        "                self.index_entries[term] = pages\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Parsed {len(self.index_entries)} index terms\")\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"File length: {len(content)} characters\")\n",
        "                print(\"First 100 characters:\", content[:100])\n",
        "                print(\"Contains \\\\begin{document}:\", '\\\\begin{document}' in content)\n",
        "                # Normalize line endings\n",
        "                content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "\n",
        "            # Find document begin and end positions\n",
        "            doc_begin = content.find('\\\\begin{document}')\n",
        "            doc_end = content.find('\\\\end{document}')\n",
        "\n",
        "            if doc_begin == -1:\n",
        "                raise ValueError(\"Could not find \\\\begin{document}. Document content:\\n\" + content[:500] + \"...\")\n",
        "\n",
        "            if doc_end == -1:\n",
        "                raise ValueError(\"Could not find \\\\end{document}\")\n",
        "\n",
        "            # Split into preamble and body\n",
        "            preamble = content[:doc_begin]\n",
        "            body = content[doc_begin:doc_end]\n",
        "            ending = content[doc_end:]\n",
        "\n",
        "            # Add required packages to preamble if not present\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                preamble = preamble.rstrip() + '\\n\\\\usepackage{makeidx}\\n'\n",
        "\n",
        "            if '\\\\makeindex' not in preamble:\n",
        "                preamble = preamble.rstrip() + '\\n\\\\makeindex\\n'\n",
        "\n",
        "            # Process the body text\n",
        "            modified_body = body\n",
        "            for term in self.index_entries.keys():\n",
        "                # Escape special LaTeX characters in the term\n",
        "                escaped_term = re.escape(term)\n",
        "                # Create pattern that matches the word boundary\n",
        "                pattern = fr'\\b{escaped_term}\\b'\n",
        "\n",
        "                # Add \\index{term} after each occurrence\n",
        "                modified_body = re.sub(\n",
        "                    pattern,\n",
        "                    f'\\\\g<0>\\\\\\\\index{{{term}}}',\n",
        "                    modified_body\n",
        "                )\n",
        "\n",
        "            # Add \\printindex before \\end{document}\n",
        "            if '\\\\printindex' not in content:\n",
        "                ending = '\\n\\\\printindex\\n' + ending\n",
        "\n",
        "            # Combine everything back\n",
        "            final_content = preamble + modified_body + ending\n",
        "\n",
        "            # Save the modified content\n",
        "            self.modified_content = final_content.split('\\n')\n",
        "\n",
        "            if self.debug:\n",
        "                # Count the number of \\index commands added\n",
        "                index_count = len(re.findall(r'\\\\index{[^}]*}', final_content))\n",
        "                print(f\"Added {index_count} index tags\")\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"Trying with different encodings...\")\n",
        "            encodings = ['latin-1', 'utf-16', 'cp1252']\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    with open(input_file, 'r', encoding=encoding) as f:\n",
        "                        content = f.read()\n",
        "                    print(f\"Successfully read file with {encoding} encoding\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "            else:\n",
        "                raise ValueError(f\"Could not read file with any of these encodings: utf-8, {', '.join(encodings)}\")\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        content = '\\n'.join(self.modified_content)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Saved modified content to {output_file}\")\n",
        "            print(\"\\nTo generate the index, run these commands in order:\")\n",
        "            print(f\"1. pdflatex {output_file}\")\n",
        "            print(f\"2. makeindex {output_file.replace('.tex', '.idx')}\")\n",
        "            print(f\"3. pdflatex {output_file}\")\n",
        "\n",
        "def process_files(tex_file_path, index_file_path, output_file_path=None):\n",
        "    \"\"\"Main function to process the files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    print(f\"Processing LaTeX file: {tex_file_path}\")\n",
        "    print(f\"Using index file: {index_file_path}\")\n",
        "    print(f\"Output will be saved to: {output_file_path}\")\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = LatexIndexer()\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Wnp09gWxlZ0d",
        "outputId": "ae4f2bd7-bf7a-4a08-9cc3-ce6f8fc19393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing LaTeX file: output-gpt-processed.tex\n",
            "Using index file: sample-indices-2.txt\n",
            "Output will be saved to: output-gpt-processed_indexed.tex\n",
            "Parsed 65 index terms\n",
            "File length: 207983 characters\n",
            "First 100 characters: \\documentclass{book} \\usepackage{graphicx} \\usepackage{amsmath} \\usepackage{amssymb} \\usepackage[dvi\n",
            "Contains \\begin{document}: True\n",
            "Added 77 index tags\n",
            "Saved modified content to output-gpt-processed_indexed.tex\n",
            "\n",
            "To generate the index, run these commands in order:\n",
            "1. pdflatex output-gpt-processed_indexed.tex\n",
            "2. makeindex output-gpt-processed_indexed.idx\n",
            "3. pdflatex output-gpt-processed_indexed.tex\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output-gpt-processed_indexed.tex'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_files('output-gpt-processed-2.tex', 'sample-indices-3.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "class PageAwareLatexIndexer:\n",
        "    def __init__(self, debug=True):\n",
        "        self.index_entries: Dict[str, Set[int]] = {}  # term -> set of pages\n",
        "        self.modified_content: List[str] = []\n",
        "        self.debug = debug\n",
        "\n",
        "    def parse_index_list(self, index_text: str) -> None:\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "\n",
        "        for line in lines:\n",
        "            # Split on the last comma to separate term and pages\n",
        "            parts = line.rsplit(',', 1)\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "\n",
        "            term, pages = parts\n",
        "            term = term.strip()\n",
        "            pages_str = pages.strip().rstrip('\\\\')\n",
        "\n",
        "            # Initialize set for this term\n",
        "            if term not in self.index_entries:\n",
        "                self.index_entries[term] = set()\n",
        "\n",
        "            # Process each page number or range\n",
        "            for page_item in pages_str.split(','):\n",
        "                page_item = page_item.strip()\n",
        "\n",
        "                # Handle page ranges (e.g., \"18-20\")\n",
        "                if '-' in page_item:\n",
        "                    start, end = map(int, page_item.split('-'))\n",
        "                    self.index_entries[term].update(range(start, end + 1))\n",
        "                else:\n",
        "                    try:\n",
        "                        self.index_entries[term].add(int(page_item))\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Parsed {len(self.index_entries)} index terms\")\n",
        "            for term, pages in self.index_entries.items():\n",
        "                print(f\"Term: {term}, Pages: {sorted(pages)}\")\n",
        "\n",
        "    def extract_page_content(self, content: str) -> Dict[int, Tuple[int, int]]:\n",
        "        \"\"\"Extract page boundaries from LaTeX content using % Page X markers.\"\"\"\n",
        "        page_positions = {}\n",
        "        current_pos = 0\n",
        "\n",
        "        while True:\n",
        "            # Find next page marker\n",
        "            marker = re.search(r'% Page (\\d+)', content[current_pos:])\n",
        "            if not marker:\n",
        "                break\n",
        "\n",
        "            page_num = int(marker.group(1))\n",
        "            start_pos = current_pos + marker.start()\n",
        "\n",
        "            # Move current position to start looking for next marker\n",
        "            current_pos = start_pos + len(marker.group(0))\n",
        "\n",
        "            # Add to page positions\n",
        "            page_positions[page_num] = start_pos\n",
        "\n",
        "        # Convert positions to ranges\n",
        "        page_ranges = {}\n",
        "        sorted_pages = sorted(page_positions.items())\n",
        "\n",
        "        for i in range(len(sorted_pages)):\n",
        "            page_num, start = sorted_pages[i]\n",
        "            end = sorted_pages[i + 1][1] if i < len(sorted_pages) - 1 else len(content)\n",
        "            page_ranges[page_num] = (start, end)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Found {len(page_ranges)} pages: {sorted(page_ranges.keys())}\")\n",
        "\n",
        "        return page_ranges\n",
        "\n",
        "    def add_index_to_content(self, content: str, term: str, start_pos: int, end_pos: int) -> str:\n",
        "        \"\"\"Add index command after the term, handling LaTeX special characters.\"\"\"\n",
        "        # Function to escape special regex characters but keep LaTeX commands\n",
        "        def escape_for_regex(s):\n",
        "            special_chars = r'[](){}?*+|^$.\\\\'\n",
        "            return ''.join('\\\\' + c if c in special_chars else c for c in s)\n",
        "\n",
        "        escaped_term = escape_for_regex(term)\n",
        "        pattern = fr'\\b{escaped_term}\\b(?![}}\\\\])'  # Negative lookahead to avoid double indexing\n",
        "\n",
        "        segment = content[start_pos:end_pos]\n",
        "        modified_segment = re.sub(\n",
        "            pattern,\n",
        "            f'\\\\g<0>\\\\\\\\index{{{term}}}',\n",
        "            segment\n",
        "        )\n",
        "\n",
        "        return content[:start_pos] + modified_segment + content[end_pos:]\n",
        "\n",
        "    def process_latex_file(self, input_file: str) -> None:\n",
        "        \"\"\"Read and process the LaTeX file with page awareness.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Normalize line endings\n",
        "            content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "\n",
        "            # Extract page positions\n",
        "            page_ranges = self.extract_page_content(content)\n",
        "\n",
        "            # Add required packages to preamble\n",
        "            doc_begin = content.find('\\\\begin{document}')\n",
        "            if doc_begin == -1:\n",
        "                raise ValueError(\"Could not find \\\\begin{document}\")\n",
        "\n",
        "            preamble = content[:doc_begin]\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                content = content[:doc_begin] + '\\\\usepackage{makeidx}\\n\\\\makeindex\\n' + content[doc_begin:]\n",
        "\n",
        "            # Process each term and its pages\n",
        "            for term, pages in self.index_entries.items():\n",
        "                if self.debug:\n",
        "                    print(f\"\\nProcessing term: {term}\")\n",
        "\n",
        "                for page in pages:\n",
        "                    if page in page_ranges:\n",
        "                        start_pos, end_pos = page_ranges[page]\n",
        "                        content = self.add_index_to_content(content, term, start_pos, end_pos)\n",
        "\n",
        "            # Add \\printindex before \\end{document}\n",
        "            if '\\\\printindex' not in content:\n",
        "                end_doc = content.find('\\\\end{document}')\n",
        "                if end_doc != -1:\n",
        "                    content = content[:end_doc] + '\\n\\\\printindex\\n' + content[end_doc:]\n",
        "\n",
        "            self.modified_content = content.split('\\n')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {e}\")\n",
        "            raise\n",
        "\n",
        "    def save_modified_file(self, output_file: str) -> None:\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        content = '\\n'.join(self.modified_content)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nSaved modified content to {output_file}\")\n",
        "            print(\"To generate the index, run these commands in order:\")\n",
        "            print(f\"1. pdflatex {output_file}\")\n",
        "            print(f\"2. makeindex {output_file.replace('.tex', '.idx')}\")\n",
        "            print(f\"3. pdflatex {output_file}\")\n",
        "\n",
        "def process_files(tex_file_path: str, index_file_path: str, output_file_path: str = None) -> str:\n",
        "    \"\"\"Process the files with page-aware indexing.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    print(f\"Processing LaTeX file: {tex_file_path}\")\n",
        "    print(f\"Using index file: {index_file_path}\")\n",
        "    print(f\"Output will be saved to: {output_file_path}\")\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = PageAwareLatexIndexer(debug=True)\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ],
      "metadata": {
        "id": "fU7wuFKZCBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_files('output-gpt-processed-2.tex', 'sample-indices-3.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V416ZKgWC4yR",
        "outputId": "6200d8f2-edb0-462f-e574-340eb103d901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing LaTeX file: output-gpt-processed-2.tex\n",
            "Using index file: sample-indices-3.txt\n",
            "Output will be saved to: output-gpt-processed-2_indexed.tex\n",
            "Parsed 18 index terms\n",
            "Term: Absolute address, Pages: [274]\n",
            "Term: add, 29, 30, 282, Pages: [334]\n",
            "Term: Addition, Pages: [314]\n",
            "Term: Addition instructions, Pages: [31]\n",
            "Term: ADDR operator, Pages: [18, 19, 20]\n",
            "Term: Aliasing, Pages: [160]\n",
            "Term: American Standard Code for Information Interchange (ASCII), 311, Pages: [312]\n",
            "Term: and, Pages: [323]\n",
            "Term: And operator (&&), Pages: [62]\n",
            "Term: Arithmetic instructions, Pages: [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
            "Term: Arithmetic shift, Pages: [105, 106, 107, 108]\n",
            "Term: . asm, Pages: [291, 292]\n",
            "Term: Array of strings, Pages: [204, 205, 206]\n",
            "Term: Arrays, Pages: [159, 160, 161, 162]\n",
            "Term: 64-bit arrays, Pages: [258, 259, 260]\n",
            "Term: floating-point arrays, Pages: [232, 233]\n",
            "Term: Assembler, Pages: [1]\n",
            "Term: Assembly language, Pages: [1, 2]\n",
            "Found 230 pages: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n",
            "\n",
            "Processing term: Absolute address\n",
            "\n",
            "Processing term: add, 29, 30, 282\n",
            "\n",
            "Processing term: Addition\n",
            "\n",
            "Processing term: Addition instructions\n",
            "\n",
            "Processing term: ADDR operator\n",
            "\n",
            "Processing term: Aliasing\n",
            "\n",
            "Processing term: American Standard Code for Information Interchange (ASCII), 311\n",
            "\n",
            "Processing term: and\n",
            "\n",
            "Processing term: And operator (&&)\n",
            "\n",
            "Processing term: Arithmetic instructions\n",
            "\n",
            "Processing term: Arithmetic shift\n",
            "\n",
            "Processing term: . asm\n",
            "\n",
            "Processing term: Array of strings\n",
            "\n",
            "Processing term: Arrays\n",
            "\n",
            "Processing term: 64-bit arrays\n",
            "\n",
            "Processing term: floating-point arrays\n",
            "\n",
            "Processing term: Assembler\n",
            "\n",
            "Processing term: Assembly language\n",
            "\n",
            "Saved modified content to output-gpt-processed-2_indexed.tex\n",
            "To generate the index, run these commands in order:\n",
            "1. pdflatex output-gpt-processed-2_indexed.tex\n",
            "2. makeindex output-gpt-processed-2_indexed.idx\n",
            "3. pdflatex output-gpt-processed-2_indexed.tex\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'output-gpt-processed-2_indexed.tex'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "class SimpleLatexIndexer:\n",
        "    def __init__(self, debug=True):\n",
        "        self.index_entries: Dict[str, Set[int]] = {}  # term -> set of pages\n",
        "        self.page_map: Dict[int, str] = {}  # page number -> content\n",
        "        self.debug = debug\n",
        "\n",
        "    def parse_index_list(self, index_text: str) -> None:\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "\n",
        "        for line in lines:\n",
        "            if ',' not in line:\n",
        "                continue\n",
        "\n",
        "            # Split on the last comma\n",
        "            term, pages = line.rsplit(',', 1)\n",
        "            term = term.strip()\n",
        "            pages_str = pages.strip().rstrip('\\\\')\n",
        "\n",
        "            if term not in self.index_entries:\n",
        "                self.index_entries[term] = set()\n",
        "\n",
        "            # Process page numbers and ranges\n",
        "            for page_item in pages_str.split(','):\n",
        "                page_item = page_item.strip()\n",
        "                if '-' in page_item:\n",
        "                    start, end = map(int, page_item.split('-'))\n",
        "                    self.index_entries[term].update(range(start, end + 1))\n",
        "                else:\n",
        "                    try:\n",
        "                        self.index_entries[term].add(int(page_item))\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nParsed {len(self.index_entries)} index terms\")\n",
        "            for term, pages in self.index_entries.items():\n",
        "                print(f\"Term: '{term}', Pages: {sorted(pages)}\")\n",
        "\n",
        "    def build_page_map(self, content: str) -> None:\n",
        "        \"\"\"Build a map of page numbers to page content.\"\"\"\n",
        "        current_page = None\n",
        "        current_content = []\n",
        "        self.page_map = {}\n",
        "\n",
        "        for line in content.split('\\n'):\n",
        "            page_match = re.match(r'% Page (\\d+)', line)\n",
        "            if page_match:\n",
        "                # Save previous page if exists\n",
        "                if current_page and current_content:\n",
        "                    self.page_map[current_page] = '\\n'.join(current_content)\n",
        "                # Start new page\n",
        "                current_page = int(page_match.group(1))\n",
        "                current_content = []\n",
        "            elif current_page is not None:\n",
        "                current_content.append(line)\n",
        "\n",
        "        # Save last page\n",
        "        if current_page and current_content:\n",
        "            self.page_map[current_page] = '\\n'.join(current_content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nBuilt page map with {len(self.page_map)} pages\")\n",
        "            print(f\"Page numbers: {sorted(self.page_map.keys())}\")\n",
        "\n",
        "    def process_latex_file(self, input_file: str) -> None:\n",
        "        \"\"\"Process the LaTeX file and add index terms.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Build page map\n",
        "            self.build_page_map(content)\n",
        "\n",
        "            # Add packages to preamble\n",
        "            doc_begin = content.find('\\\\begin{document}')\n",
        "            if doc_begin != -1:\n",
        "                preamble = content[:doc_begin]\n",
        "                if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                    package_text = '\\\\usepackage{makeidx}\\n\\\\makeindex\\n'\n",
        "                    content = content[:doc_begin] + package_text + content[doc_begin:]\n",
        "\n",
        "            # Process each index entry\n",
        "            modified_content = content\n",
        "            for term, pages in self.index_entries.items():\n",
        "                for page_num in pages:\n",
        "                    if page_num in self.page_map:\n",
        "                        page_content = self.page_map[page_num]\n",
        "\n",
        "                        # Find a good insertion point - about 1/3 through the page\n",
        "                        lines = page_content.split('\\n')\n",
        "                        insert_pos = len(lines) // 3\n",
        "\n",
        "                        # Add the index command on its own line\n",
        "                        lines.insert(insert_pos, f\"\\\\index{{{term}}}\")\n",
        "                        modified_page = '\\n'.join(lines)\n",
        "\n",
        "                        # Replace the original page content\n",
        "                        modified_content = modified_content.replace(page_content, modified_page)\n",
        "\n",
        "                        if self.debug:\n",
        "                            print(f\"Added index for '{term}' on page {page_num}\")\n",
        "\n",
        "            # Add \\printindex if not present\n",
        "            if '\\\\printindex' not in modified_content:\n",
        "                end_doc = modified_content.find('\\\\end{document}')\n",
        "                if end_doc != -1:\n",
        "                    modified_content = (\n",
        "                        modified_content[:end_doc] +\n",
        "                        '\\n\\\\printindex\\n\\n' +\n",
        "                        modified_content[end_doc:]\n",
        "                    )\n",
        "\n",
        "            self.modified_content = modified_content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {e}\")\n",
        "            raise\n",
        "\n",
        "    def save_modified_file(self, output_file: str) -> None:\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(self.modified_content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nSaved modified content to {output_file}\")\n",
        "            print(\"\\nTo generate the index, run:\")\n",
        "            print(f\"1. pdflatex {output_file}\")\n",
        "            print(f\"2. makeindex {output_file.replace('.tex', '.idx')}\")\n",
        "            print(f\"3. pdflatex {output_file}\")\n",
        "\n",
        "def process_files(tex_file_path: str, index_file_path: str, output_file_path: str = None) -> str:\n",
        "    \"\"\"Process the files with the simple indexing approach.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    print(f\"\\nProcessing files:\")\n",
        "    print(f\"LaTeX file: {tex_file_path}\")\n",
        "    print(f\"Index file: {index_file_path}\")\n",
        "    print(f\"Output file: {output_file_path}\")\n",
        "\n",
        "    indexer = SimpleLatexIndexer(debug=True)\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ],
      "metadata": {
        "id": "ujwcnnJ8Ft_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_files('output-gpt-processed-2.tex', 'sample-indices-3.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Y2mqfWfFvKm",
        "outputId": "9791470f-7403-41ba-8b8b-420f54c69ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing files:\n",
            "LaTeX file: output-gpt-processed-2.tex\n",
            "Index file: sample-indices-3.txt\n",
            "Output file: output-gpt-processed-2_indexed.tex\n",
            "\n",
            "Parsed 18 index terms\n",
            "Term: 'Absolute address', Pages: [274]\n",
            "Term: 'add, 29, 30, 282', Pages: [334]\n",
            "Term: 'Addition', Pages: [314]\n",
            "Term: 'Addition instructions', Pages: [31]\n",
            "Term: 'ADDR operator', Pages: [18, 19, 20]\n",
            "Term: 'Aliasing', Pages: [160]\n",
            "Term: 'American Standard Code for Information Interchange (ASCII), 311', Pages: [312]\n",
            "Term: 'and', Pages: [323]\n",
            "Term: 'And operator (&&)', Pages: [62]\n",
            "Term: 'Arithmetic instructions', Pages: [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]\n",
            "Term: 'Arithmetic shift', Pages: [105, 106, 107, 108]\n",
            "Term: '. asm', Pages: [291, 292]\n",
            "Term: 'Array of strings', Pages: [204, 205, 206]\n",
            "Term: 'Arrays', Pages: [159, 160, 161, 162]\n",
            "Term: '64-bit arrays', Pages: [258, 259, 260]\n",
            "Term: 'floating-point arrays', Pages: [232, 233]\n",
            "Term: 'Assembler', Pages: [1]\n",
            "Term: 'Assembly language', Pages: [1, 2]\n",
            "\n",
            "Built page map with 230 pages\n",
            "Page numbers: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n",
            "Added index for 'Addition instructions' on page 31\n",
            "Added index for 'ADDR operator' on page 18\n",
            "Added index for 'ADDR operator' on page 19\n",
            "Added index for 'ADDR operator' on page 20\n",
            "Added index for 'Aliasing' on page 160\n",
            "Added index for 'And operator (&&)' on page 62\n",
            "Added index for 'Arithmetic instructions' on page 32\n",
            "Added index for 'Arithmetic instructions' on page 33\n",
            "Added index for 'Arithmetic instructions' on page 34\n",
            "Added index for 'Arithmetic instructions' on page 35\n",
            "Added index for 'Arithmetic instructions' on page 36\n",
            "Added index for 'Arithmetic instructions' on page 37\n",
            "Added index for 'Arithmetic instructions' on page 38\n",
            "Added index for 'Arithmetic instructions' on page 39\n",
            "Added index for 'Arithmetic instructions' on page 40\n",
            "Added index for 'Arithmetic instructions' on page 41\n",
            "Added index for 'Arithmetic instructions' on page 42\n",
            "Added index for 'Arithmetic instructions' on page 43\n",
            "Added index for 'Arithmetic instructions' on page 44\n",
            "Added index for 'Arithmetic instructions' on page 45\n",
            "Added index for 'Arithmetic instructions' on page 46\n",
            "Added index for 'Arithmetic instructions' on page 29\n",
            "Added index for 'Arithmetic instructions' on page 30\n",
            "Added index for 'Arithmetic instructions' on page 31\n",
            "Added index for 'Arithmetic shift' on page 105\n",
            "Added index for 'Arithmetic shift' on page 106\n",
            "Added index for 'Arithmetic shift' on page 107\n",
            "Added index for 'Arithmetic shift' on page 108\n",
            "Added index for 'Array of strings' on page 204\n",
            "Added index for 'Array of strings' on page 205\n",
            "Added index for 'Array of strings' on page 206\n",
            "Added index for 'Arrays' on page 160\n",
            "Added index for 'Arrays' on page 161\n",
            "Added index for 'Arrays' on page 162\n",
            "Added index for 'Arrays' on page 159\n",
            "Added index for 'floating-point arrays' on page 232\n",
            "Added index for 'floating-point arrays' on page 233\n",
            "\n",
            "Saved modified content to output-gpt-processed-2_indexed.tex\n",
            "\n",
            "To generate the index, run:\n",
            "1. pdflatex output-gpt-processed-2_indexed.tex\n",
            "2. makeindex output-gpt-processed-2_indexed.idx\n",
            "3. pdflatex output-gpt-processed-2_indexed.tex\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'output-gpt-processed-2_indexed.tex'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnXBsWXtnpg0",
        "outputId": "9180534c-5265-4c81-8851-cccdd5e6debd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: pdflatex: command not found\n"
          ]
        }
      ],
      "source": [
        "!pdflatex output-gpt-processed_indexed.tex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_ElT0kpfGMc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "class LatexIndexer:\n",
        "    def __init__(self):\n",
        "        self.index_entries = {}\n",
        "        self.modified_content = []\n",
        "\n",
        "    def parse_index_list(self, index_text):\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "        lines = [line for line in lines if not line.startswith('\\\\begin') and not line.startswith('\\\\end')]\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'(.*?),\\s*((?:\\d+(?:,\\s*\\d+)*))\\\\?$', line)\n",
        "            if match:\n",
        "                term, pages = match.groups()\n",
        "                term = term.strip()\n",
        "                # Store as strings to preserve the exact page numbers we want in the index\n",
        "                pages = [p.strip() for p in pages.split(',')]\n",
        "                self.index_entries[term] = pages\n",
        "\n",
        "    def find_term_in_text(self, text, term):\n",
        "        \"\"\"Find exact occurrences of the term in text, ignoring LaTeX commands.\"\"\"\n",
        "        escaped_term = re.escape(term).replace('\\\\ ', ' ')\n",
        "        pattern = rf'\\b{escaped_term}\\b'\n",
        "        return list(re.finditer(pattern, text))\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        lines = content.split('\\n')\n",
        "        modified_lines = lines.copy()\n",
        "\n",
        "        # Process each term\n",
        "        for term, pages in self.index_entries.items():\n",
        "            # Find all occurrences of the term\n",
        "            content_so_far = ''\n",
        "            for i, line in enumerate(lines):\n",
        "                content_so_far += line + '\\n'\n",
        "                matches = self.find_term_in_text(line, term)\n",
        "                if matches:\n",
        "                    # Add index command after the term\n",
        "                    for match in matches:\n",
        "                        modified_line = modified_lines[i]\n",
        "                        index_cmd = f'\\\\index{{{term}}}'\n",
        "                        pos = match.end()\n",
        "                        modified_lines[i] = (\n",
        "                            modified_line[:pos] +\n",
        "                            index_cmd +\n",
        "                            modified_line[pos:]\n",
        "                        )\n",
        "\n",
        "        self.modified_content = modified_lines\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(self.modified_content))\n",
        "\n",
        "    def add_index_package(self):\n",
        "        \"\"\"Add required index packages if not present.\"\"\"\n",
        "        preamble_end = -1\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\begin{document}' in line:\n",
        "                preamble_end = i\n",
        "                break\n",
        "\n",
        "        if preamble_end != -1:\n",
        "            preamble = '\\n'.join(self.modified_content[:preamble_end])\n",
        "            packages_to_add = []\n",
        "\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                packages_to_add.append('\\\\usepackage{makeidx}')\n",
        "            if '\\\\makeindex' not in preamble:\n",
        "                packages_to_add.append('\\\\makeindex')\n",
        "\n",
        "            if packages_to_add:\n",
        "                self.modified_content[preamble_end:preamble_end] = packages_to_add\n",
        "\n",
        "        # Add \\printindex before \\end{document} if not present\n",
        "        doc_end = len(self.modified_content)\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\end{document}' in line:\n",
        "                doc_end = i\n",
        "                break\n",
        "\n",
        "        if '\\\\printindex' not in '\\n'.join(self.modified_content):\n",
        "            self.modified_content.insert(doc_end, '\\\\printindex')\n",
        "\n",
        "def process_files(tex_file_path, index_file_path, output_file_path=None):\n",
        "    \"\"\"Main function to process the files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = LatexIndexer()\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.add_index_package()\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4Lcl5ePsEHo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "class Match:\n",
        "    \"\"\"Custom match object to handle both exact and fuzzy matches\"\"\"\n",
        "    def __init__(self, start_pos, end_pos, matched_text, similarity=1.0):\n",
        "        self._start = start_pos\n",
        "        self._end = end_pos\n",
        "        self._text = matched_text\n",
        "        self.similarity = similarity\n",
        "\n",
        "    def start(self):\n",
        "        return self._start\n",
        "\n",
        "    def end(self):\n",
        "        return self._end\n",
        "\n",
        "    def group(self):\n",
        "        return self._text\n",
        "\n",
        "class LatexIndexer:\n",
        "    def __init__(self, similarity_threshold=0.8):\n",
        "        self.index_entries = {}\n",
        "        self.modified_content = []\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def parse_index_list(self, index_text):\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "        lines = [line for line in lines if not line.startswith('\\\\begin') and not line.startswith('\\\\end')]\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'(.*?),\\s*((?:\\d+(?:,\\s*\\d+)*))\\\\?$', line)\n",
        "            if match:\n",
        "                term, pages = match.groups()\n",
        "                term = term.strip()\n",
        "                pages = [p.strip() for p in pages.split(',')]\n",
        "                self.index_entries[term] = pages\n",
        "\n",
        "    def calculate_similarity(self, str1, str2):\n",
        "        \"\"\"Calculate similarity ratio between two strings.\"\"\"\n",
        "        return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
        "\n",
        "    def find_best_match_in_text(self, text, term):\n",
        "        \"\"\"Find the best matching substring for the term in text.\"\"\"\n",
        "        # First try exact match\n",
        "        escaped_term = re.escape(term).replace('\\\\ ', ' ')\n",
        "        pattern = rf'\\b{escaped_term}\\b'\n",
        "\n",
        "        exact_matches = []\n",
        "        for m in re.finditer(pattern, text):\n",
        "            # Convert re.Match to our custom Match\n",
        "            exact_matches.append(Match(m.start(), m.end(), m.group(), 1.0))\n",
        "\n",
        "        if exact_matches:\n",
        "            return exact_matches\n",
        "\n",
        "        # If no exact match, try fuzzy matching\n",
        "        words = text.split()\n",
        "        best_matches = []\n",
        "\n",
        "        # Look for matches in sliding windows of various sizes\n",
        "        term_word_count = len(term.split())\n",
        "        window_sizes = range(max(1, term_word_count - 1), term_word_count + 2)\n",
        "\n",
        "        for window_size in window_sizes:\n",
        "            for i in range(len(words) - window_size + 1):\n",
        "                window = ' '.join(words[i:i + window_size])\n",
        "                similarity = self.calculate_similarity(term, window)\n",
        "\n",
        "                if similarity >= self.similarity_threshold:\n",
        "                    # Calculate position in original text\n",
        "                    start_pos = len(' '.join(words[:i]))\n",
        "                    if i > 0:\n",
        "                        start_pos += 1  # Add space if not at beginning\n",
        "                    end_pos = start_pos + len(window)\n",
        "\n",
        "                    best_matches.append(Match(start_pos, end_pos, window, similarity))\n",
        "\n",
        "        # Sort by similarity and return the best matches\n",
        "        return sorted(best_matches, key=lambda x: x.similarity, reverse=True)\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        lines = content.split('\\n')\n",
        "        modified_lines = lines.copy()\n",
        "        unmatched_terms = []\n",
        "\n",
        "        # Process each term\n",
        "        for term, pages in self.index_entries.items():\n",
        "            term_found = False\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                # Skip lines with LaTeX commands\n",
        "                if line.strip().startswith('\\\\'):\n",
        "                    continue\n",
        "\n",
        "                matches = self.find_best_match_in_text(line, term)\n",
        "\n",
        "                if matches:\n",
        "                    term_found = True\n",
        "                    modified_line = modified_lines[i]\n",
        "                    for match in matches:\n",
        "                        index_cmd = f'\\\\index{{{term}}}'\n",
        "                        pos = match.end()\n",
        "                        modified_lines[i] = (\n",
        "                            modified_line[:pos] +\n",
        "                            index_cmd +\n",
        "                            modified_line[pos:]\n",
        "                        )\n",
        "                        # Only use the first match if it's a fuzzy match\n",
        "                        if match.similarity < 1.0:\n",
        "                            break\n",
        "\n",
        "            if not term_found:\n",
        "                unmatched_terms.append(term)\n",
        "\n",
        "        self.modified_content = modified_lines\n",
        "        if unmatched_terms:\n",
        "            print(\"Warning: The following terms could not be matched:\")\n",
        "            for term in unmatched_terms:\n",
        "                print(f\"  - {term}\")\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(self.modified_content))\n",
        "\n",
        "    def add_index_package(self):\n",
        "        \"\"\"Add required index packages if not present.\"\"\"\n",
        "        preamble_end = -1\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\begin{document}' in line:\n",
        "                preamble_end = i\n",
        "                break\n",
        "\n",
        "        if preamble_end != -1:\n",
        "            preamble = '\\n'.join(self.modified_content[:preamble_end])\n",
        "            packages_to_add = []\n",
        "\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                packages_to_add.append('\\\\usepackage{makeidx}')\n",
        "            if '\\\\makeindex' not in preamble:\n",
        "                packages_to_add.append('\\\\makeindex')\n",
        "\n",
        "            if packages_to_add:\n",
        "                self.modified_content[preamble_end:preamble_end] = packages_to_add\n",
        "\n",
        "        # Add \\printindex before \\end{document} if not present\n",
        "        doc_end = len(self.modified_content)\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\end{document}' in line:\n",
        "                doc_end = i\n",
        "                break\n",
        "\n",
        "        if '\\\\printindex' not in '\\n'.join(self.modified_content):\n",
        "            self.modified_content.insert(doc_end, '\\\\printindex')\n",
        "\n",
        "def process_files(tex_file_path, index_file_path, output_file_path=None, similarity_threshold=0.8):\n",
        "    \"\"\"Main function to process the files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = LatexIndexer(similarity_threshold=similarity_threshold)\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.add_index_package()\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSPY-e20tLMM",
        "outputId": "94885516-915e-4b0e-88c7-3922ebd348e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqyGevabtP86"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive/pdf2latex/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "0ir49dzvhWv2",
        "outputId": "097480bf-2999-4e14-941d-1ce98f37a890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: The following terms could not be matched:\n",
            "  - A/B testing\n",
            "  - academic data\n",
            "  - accuracy\n",
            "  - activation function\n",
            "  - AdaBoost\n",
            "  - add-one discounting\n",
            "  - agglomerative cluster trees\n",
            "  - Akaike information criterion\n",
            "  - algorithm analysis\n",
            "  - Turkers\n",
            "  - American basketball players\n",
            "  - analogies\n",
            "  - anchoring\n",
            "  - angular distance\n",
            "  - Anscombe's Quartet\n",
            "  - Apple iPhone sales\n",
            "  - Aristotle\n",
            "  - Arrow's impossibility theorem\n",
            "  - Ascombe quartet\n",
            "  - associativity\n",
            "  - Babbage, Charles\n",
            "  - backpropagation\n",
            "  - Bacon, Kevin\n",
            "  - balanced training classes\n",
            "  - bar charts\n",
            "  - Barzun, Jacques\n",
            "  - baseball encyclopedia\n",
            "  - baseline models\n",
            "  - for value prediction\n",
            "  - Baysian information criteria\n",
            "  - lexicographic\n",
            "  - temporal\n",
            "  - bias–variance trade-off\n",
            "  - big oh analysis\n",
            "  - binary relations\n",
            "  - binary search\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output-gpt-processed_indexed.tex'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_files('output-gpt-processed.tex', 'sample-indices-2.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kHEvNb8lGry"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "class Match:\n",
        "    def __init__(self, start_pos, end_pos, matched_text, page_num=None, similarity=1.0):\n",
        "        self._start = start_pos\n",
        "        self._end = end_pos\n",
        "        self._text = matched_text\n",
        "        self.page_num = page_num\n",
        "        self.similarity = similarity\n",
        "\n",
        "    def start(self):\n",
        "        return self._start\n",
        "\n",
        "    def end(self):\n",
        "        return self._end\n",
        "\n",
        "    def group(self):\n",
        "        return self._text\n",
        "\n",
        "class LatexIndexer:\n",
        "    def __init__(self, similarity_threshold=0.8):\n",
        "        self.index_entries = {}  # Will store {term: [page_numbers]}\n",
        "        self.modified_content = []\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.page_boundaries = {}  # Will store {page_number: (start_line, end_line)}\n",
        "\n",
        "    def parse_index_list(self, index_text):\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "        lines = [line for line in lines if not line.startswith('\\\\begin') and not line.startswith('\\\\end')]\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'(.*?),\\s*((?:\\d+(?:,\\s*\\d+)*))\\\\?$', line)\n",
        "            if match:\n",
        "                term, pages = match.groups()\n",
        "                term = term.strip()\n",
        "                # Convert page numbers to integers\n",
        "                pages = [int(p.strip()) for p in pages.split(',')]\n",
        "                self.index_entries[term] = sorted(pages)\n",
        "                print(f\"Parsed term: {term} with pages: {pages}\")  # Debug output\n",
        "\n",
        "    def find_page_boundaries(self, content):\n",
        "        \"\"\"Find the line ranges for each page in the document.\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        current_page = 1\n",
        "        start_line = 0\n",
        "\n",
        "        # Common LaTeX page break commands\n",
        "        page_breaks = [\n",
        "            r'\\\\newpage',\n",
        "            r'\\\\pagebreak',\n",
        "            r'\\\\clearpage',\n",
        "            r'\\\\cleardoublepage'\n",
        "        ]\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            # Check for page number setting commands\n",
        "            page_set = re.search(r'\\\\setcounter{page}{(\\d+)}', line)\n",
        "            if page_set:\n",
        "                current_page = int(page_set.group(1))\n",
        "                if start_line < i:\n",
        "                    self.page_boundaries[current_page-1] = (start_line, i)\n",
        "                start_line = i\n",
        "\n",
        "            # Check for page breaks\n",
        "            if any(re.search(break_cmd, line) for break_cmd in page_breaks):\n",
        "                self.page_boundaries[current_page] = (start_line, i)\n",
        "                current_page += 1\n",
        "                start_line = i + 1\n",
        "\n",
        "        # Add the last page\n",
        "        self.page_boundaries[current_page] = (start_line, len(lines))\n",
        "        print(f\"Found page boundaries: {self.page_boundaries}\")  # Debug output\n",
        "\n",
        "    def is_line_in_pages(self, line_num, target_pages):\n",
        "        \"\"\"Check if a line falls within any of the target pages.\"\"\"\n",
        "        for page, (start, end) in self.page_boundaries.items():\n",
        "            if start <= line_num <= end and page in target_pages:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # First find page boundaries\n",
        "        self.find_page_boundaries(content)\n",
        "\n",
        "        lines = content.split('\\n')\n",
        "        modified_lines = lines.copy()\n",
        "        unmatched_terms = []\n",
        "\n",
        "        # Process each term\n",
        "        for term, target_pages in self.index_entries.items():\n",
        "            term_found = False\n",
        "\n",
        "            # Only process lines within the specified pages\n",
        "            for i, line in enumerate(lines):\n",
        "                # Skip if line is not in target pages\n",
        "                if not self.is_line_in_pages(i, target_pages):\n",
        "                    continue\n",
        "\n",
        "                # Skip lines with LaTeX commands\n",
        "                if line.strip().startswith('\\\\'):\n",
        "                    continue\n",
        "\n",
        "                matches = self.find_best_match_in_text(line, term)\n",
        "\n",
        "                if matches:\n",
        "                    term_found = True\n",
        "                    modified_line = modified_lines[i]\n",
        "                    for match in matches:\n",
        "                        # Get current page number for this match\n",
        "                        current_page = None\n",
        "                        for page, (start, end) in self.page_boundaries.items():\n",
        "                            if start <= i <= end:\n",
        "                                current_page = page\n",
        "                                break\n",
        "\n",
        "                        if current_page in target_pages:\n",
        "                            index_cmd = f'\\\\index{{{term}}}'\n",
        "                            pos = match.end()\n",
        "                            modified_lines[i] = (\n",
        "                                modified_line[:pos] +\n",
        "                                index_cmd +\n",
        "                                modified_line[pos:]\n",
        "                            )\n",
        "                            # Only use the first match if it's a fuzzy match\n",
        "                            if match.similarity < 1.0:\n",
        "                                break\n",
        "\n",
        "            if not term_found:\n",
        "                unmatched_terms.append(f\"{term} (pages {target_pages})\")\n",
        "\n",
        "        self.modified_content = modified_lines\n",
        "        if unmatched_terms:\n",
        "            print(\"Warning: The following terms could not be matched on their specified pages:\")\n",
        "            for term in unmatched_terms:\n",
        "                print(f\"  - {term}\")\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(self.modified_content))\n",
        "\n",
        "    def add_index_package(self):\n",
        "        \"\"\"Add required index packages if not present.\"\"\"\n",
        "        preamble_end = -1\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\begin{document}' in line:\n",
        "                preamble_end = i\n",
        "                break\n",
        "\n",
        "        if preamble_end != -1:\n",
        "            preamble = '\\n'.join(self.modified_content[:preamble_end])\n",
        "            packages_to_add = []\n",
        "\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                packages_to_add.append('\\\\usepackage{makeidx}')\n",
        "            if '\\\\makeindex' not in preamble:\n",
        "                packages_to_add.append('\\\\makeindex')\n",
        "\n",
        "            if packages_to_add:\n",
        "                self.modified_content[preamble_end:preamble_end] = packages_to_add\n",
        "\n",
        "        # Add \\printindex before \\end{document} if not present\n",
        "        doc_end = len(self.modified_content)\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\end{document}' in line:\n",
        "                doc_end = i\n",
        "                break\n",
        "\n",
        "        if '\\\\printindex' not in '\\n'.join(self.modified_content):\n",
        "            self.modified_content.insert(doc_end, '\\\\printindex')\n",
        "\n",
        "\n",
        "    def calculate_similarity(self, str1, str2):\n",
        "        \"\"\"Calculate similarity ratio between two strings.\"\"\"\n",
        "        return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
        "\n",
        "    def find_best_match_in_text(self, text, term):\n",
        "        \"\"\"Find the best matching substring for the term in text.\"\"\"\n",
        "        # First try exact match\n",
        "        escaped_term = re.escape(term).replace('\\\\ ', ' ')\n",
        "        pattern = rf'\\b{escaped_term}\\b'\n",
        "\n",
        "        exact_matches = []\n",
        "        for m in re.finditer(pattern, text):\n",
        "            # Convert re.Match to our custom Match\n",
        "            exact_matches.append(Match(m.start(), m.end(), m.group(), 1.0))\n",
        "\n",
        "        if exact_matches:\n",
        "            return exact_matches\n",
        "\n",
        "        # If no exact match, try fuzzy matching\n",
        "        words = text.split()\n",
        "        best_matches = []\n",
        "\n",
        "        # Look for matches in sliding windows of various sizes\n",
        "        term_word_count = len(term.split())\n",
        "        window_sizes = range(max(1, term_word_count - 1), term_word_count + 2)\n",
        "\n",
        "        for window_size in window_sizes:\n",
        "            for i in range(len(words) - window_size + 1):\n",
        "                window = ' '.join(words[i:i + window_size])\n",
        "                similarity = self.calculate_similarity(term, window)\n",
        "\n",
        "                if similarity >= self.similarity_threshold:\n",
        "                    # Calculate position in original text\n",
        "                    start_pos = len(' '.join(words[:i]))\n",
        "                    if i > 0:\n",
        "                        start_pos += 1  # Add space if not at beginning\n",
        "                    end_pos = start_pos + len(window)\n",
        "\n",
        "                    best_matches.append(Match(start_pos, end_pos, window, similarity))\n",
        "\n",
        "        # Sort by similarity and return the best matches\n",
        "        return sorted(best_matches, key=lambda x: x.similarity, reverse=True)\n",
        "\n",
        "def process_files(tex_file_path, index_file_path, output_file_path=None, similarity_threshold=0.8):\n",
        "    \"\"\"Main function to process the files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = LatexIndexer(similarity_threshold=similarity_threshold)\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.add_index_package()\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NE-nReZhlTlV",
        "outputId": "5b0806ba-3eab-401d-8a73-49e2bc2d7905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed term: A/B testing with pages: [86]\n",
            "Parsed term: Aaron Schwartz case with pages: [68]\n",
            "Parsed term: AB testing with pages: [137]\n",
            "Parsed term: academic data with pages: [66]\n",
            "Parsed term: accuracy with pages: [215, 228]\n",
            "Parsed term: activation function with pages: [380]\n",
            "Parsed term: AdaBoost with pages: [364]\n",
            "Parsed term: add-one discounting with pages: [357]\n",
            "Parsed term: agglomerative cluster trees with pages: [338]\n",
            "Parsed term: aggregation mechanisms with pages: [83]\n",
            "Parsed term: Akaike information criterion with pages: [289, 335]\n",
            "Parsed term: algorithm analysis with pages: [397]\n",
            "Parsed term: Amazon Turk with pages: [67, 84]\n",
            "Parsed term: tasks assigned with pages: [85]\n",
            "Parsed term: Turkers with pages: [84]\n",
            "Parsed term: American basketball players with pages: [97]\n",
            "Parsed term: analogies with pages: [312]\n",
            "Parsed term: anchoring with pages: [82]\n",
            "Parsed term: angular distance with pages: [310]\n",
            "Parsed term: Anscombe's Quartet with pages: [159]\n",
            "Parsed term: AOL with pages: [64]\n",
            "Parsed term: API with pages: [65]\n",
            "Parsed term: Apple iPhone sales with pages: [34]\n",
            "Parsed term: application program interfaces with pages: [65]\n",
            "Parsed term: area under the ROC curve with pages: [219]\n",
            "Parsed term: Aristotle with pages: [326, 327]\n",
            "Parsed term: Arrow's impossibility theorem with pages: [84, 114]\n",
            "Parsed term: artifacts with pages: [69]\n",
            "Parsed term: Ascombe quartet with pages: [272]\n",
            "Parsed term: asking interesting questions with pages: [4]\n",
            "Parsed term: associativity with pages: [244]\n",
            "Parsed term: autocorrelation with pages: [46]\n",
            "Parsed term: average link with pages: [340]\n",
            "Parsed term: Babbage, Charles with pages: [57, 90]\n",
            "Parsed term: backpropagation with pages: [382]\n",
            "Parsed term: Bacon, Kevin with pages: [9]\n",
            "Parsed term: bag of words with pages: [14]\n",
            "Parsed term: bagging with pages: [362]\n",
            "Parsed term: balanced training classes with pages: [295]\n",
            "Parsed term: bar charts with pages: [179]\n",
            "Parsed term: best practices with pages: [181]\n",
            "Parsed term: stacked with pages: [181]\n",
            "Parsed term: Barzun, Jacques with pages: [5]\n",
            "Parsed term: baseball encyclopedia with pages: [5]\n",
            "Parsed term: baseline models with pages: [210]\n",
            "Parsed term: for classification with pages: [210]\n",
            "Parsed term: for value prediction with pages: [212]\n",
            "Parsed term: Bayes' theorem with pages: [150, 205, 299]\n",
            "Parsed term: Baysian information criteria with pages: [289]\n",
            "Parsed term: bell-shaped with pages: [123, 141]\n",
            "Parsed term: distribution with pages: [101]\n",
            "Parsed term: bias with pages: [202, 417]\n",
            "Parsed term: lexicographic with pages: [405]\n",
            "Parsed term: numerical with pages: [405]\n",
            "Parsed term: temporal with pages: [405]\n",
            "Parsed term: bias–variance trade-off with pages: [202]\n",
            "Parsed term: big data with pages: [391]\n",
            "Parsed term: algorithms with pages: [397]\n",
            "Parsed term: bad data with pages: [392]\n",
            "Parsed term: statistics with pages: [392]\n",
            "Parsed term: big data engineer with pages: [4]\n",
            "Parsed term: big oh analysis with pages: [397]\n",
            "Parsed term: binary relations with pages: [320]\n",
            "Parsed term: binary search with pages: [398]\n",
            "Parsed term: binomial distribution with pages: [123]\n",
            "Found page boundaries: {20: (0, 69), 32: (945, 951), 28: (287, 633), 29: (633, 792), 30: (793, 821), 31: (822, 944), 66: (952, 1075), 67: (1075, 1283), 68: (1284, 1782), 69: (1783, 1884)}\n",
            "Warning: The following terms could not be matched on their specified pages:\n",
            "  - A/B testing (pages [86])\n",
            "  - AB testing (pages [137])\n",
            "  - academic data (pages [66])\n",
            "  - accuracy (pages [215, 228])\n",
            "  - activation function (pages [380])\n",
            "  - AdaBoost (pages [364])\n",
            "  - add-one discounting (pages [357])\n",
            "  - agglomerative cluster trees (pages [338])\n",
            "  - aggregation mechanisms (pages [83])\n",
            "  - Akaike information criterion (pages [289, 335])\n",
            "  - algorithm analysis (pages [397])\n",
            "  - Amazon Turk (pages [67, 84])\n",
            "  - tasks assigned (pages [85])\n",
            "  - Turkers (pages [84])\n",
            "  - American basketball players (pages [97])\n",
            "  - analogies (pages [312])\n",
            "  - anchoring (pages [82])\n",
            "  - angular distance (pages [310])\n",
            "  - Anscombe's Quartet (pages [159])\n",
            "  - AOL (pages [64])\n",
            "  - API (pages [65])\n",
            "  - Apple iPhone sales (pages [34])\n",
            "  - application program interfaces (pages [65])\n",
            "  - area under the ROC curve (pages [219])\n",
            "  - Aristotle (pages [326, 327])\n",
            "  - Arrow's impossibility theorem (pages [84, 114])\n",
            "  - Ascombe quartet (pages [272])\n",
            "  - asking interesting questions (pages [4])\n",
            "  - associativity (pages [244])\n",
            "  - autocorrelation (pages [46])\n",
            "  - average link (pages [340])\n",
            "  - Babbage, Charles (pages [57, 90])\n",
            "  - backpropagation (pages [382])\n",
            "  - Bacon, Kevin (pages [9])\n",
            "  - bag of words (pages [14])\n",
            "  - bagging (pages [362])\n",
            "  - balanced training classes (pages [295])\n",
            "  - bar charts (pages [179])\n",
            "  - best practices (pages [181])\n",
            "  - stacked (pages [181])\n",
            "  - Barzun, Jacques (pages [5])\n",
            "  - baseball encyclopedia (pages [5])\n",
            "  - baseline models (pages [210])\n",
            "  - for classification (pages [210])\n",
            "  - for value prediction (pages [212])\n",
            "  - Bayes' theorem (pages [150, 205, 299])\n",
            "  - Baysian information criteria (pages [289])\n",
            "  - bell-shaped (pages [123, 141])\n",
            "  - distribution (pages [101])\n",
            "  - bias (pages [202, 417])\n",
            "  - lexicographic (pages [405])\n",
            "  - numerical (pages [405])\n",
            "  - temporal (pages [405])\n",
            "  - bias–variance trade-off (pages [202])\n",
            "  - big data (pages [391])\n",
            "  - algorithms (pages [397])\n",
            "  - bad data (pages [392])\n",
            "  - statistics (pages [392])\n",
            "  - big data engineer (pages [4])\n",
            "  - big oh analysis (pages [397])\n",
            "  - binary relations (pages [320])\n",
            "  - binary search (pages [398])\n",
            "  - binomial distribution (pages [123])\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output-gpt-processed_indexed.tex'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_files('output-gpt-processed.tex', 'sample-indices-2.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTmS2Jt0mmvZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "class Match:\n",
        "    def __init__(self, start_pos, end_pos, matched_text, page_num=None, similarity=1.0):\n",
        "        self._start = start_pos\n",
        "        self._end = end_pos\n",
        "        self._text = matched_text\n",
        "        self.page_num = page_num\n",
        "        self.similarity = similarity\n",
        "\n",
        "    def start(self):\n",
        "        return self._start\n",
        "\n",
        "    def end(self):\n",
        "        return self._end\n",
        "\n",
        "    def group(self):\n",
        "        return self._text\n",
        "\n",
        "class LatexIndexer:\n",
        "    def __init__(self, similarity_threshold=0.8):\n",
        "        self.index_entries = {}\n",
        "        self.modified_content = []\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.page_boundaries = {}\n",
        "        self.debug = True\n",
        "        self.added_terms_tracking = {}\n",
        "\n",
        "    def parse_index_list(self, index_text):\n",
        "        \"\"\"Parse the index list into a dictionary of terms and their pages.\"\"\"\n",
        "        lines = [line.strip() for line in index_text.split('\\n') if line.strip()]\n",
        "        lines = [line for line in lines if not line.startswith('\\\\begin') and not line.startswith('\\\\end')]\n",
        "\n",
        "        for line in lines:\n",
        "            match = re.match(r'(.*?),\\s*((?:\\d+(?:,\\s*\\d+)*))\\\\?$', line)\n",
        "            if match:\n",
        "                term, pages = match.groups()\n",
        "                term = term.strip()\n",
        "                pages = [int(p.strip()) for p in pages.split(',')]\n",
        "                self.index_entries[term] = sorted(pages)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Parsed {len(self.index_entries)} index terms\")\n",
        "\n",
        "    def find_term_in_text(self, term, text, start_pos=0):\n",
        "        \"\"\"Find term in text with fuzzy matching.\"\"\"\n",
        "        best_match = None\n",
        "        best_ratio = 0\n",
        "\n",
        "        clean_text = re.sub(r'\\\\[a-zA-Z]+(\\{[^}]*\\})?', '', text)\n",
        "\n",
        "        words = clean_text[start_pos:].split()\n",
        "        for i in range(len(words)):\n",
        "            for j in range(i + 1, len(words) + 1):\n",
        "                candidate = ' '.join(words[i:j])\n",
        "                ratio = SequenceMatcher(None, term.lower(), candidate.lower()).ratio()\n",
        "\n",
        "                if ratio > best_ratio and ratio >= self.similarity_threshold:\n",
        "                    start = text[start_pos:].find(candidate) + start_pos\n",
        "                    end = start + len(candidate)\n",
        "                    best_match = Match(start, end, candidate, similarity=ratio)\n",
        "                    best_ratio = ratio\n",
        "\n",
        "        return best_match\n",
        "\n",
        "    def get_page_content(self, page_num):\n",
        "        \"\"\"Get content for a specific page.\"\"\"\n",
        "        if page_num in self.page_boundaries:\n",
        "            start_line, end_line = self.page_boundaries[page_num]\n",
        "            return '\\n'.join(self.modified_content[start_line:end_line])\n",
        "        return \"\"\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            self.modified_content = content.split('\\n')\n",
        "            self.estimate_page_boundaries(content)\n",
        "\n",
        "            # Initialize tracking for each term and page\n",
        "            self.added_terms_tracking = {\n",
        "                term: {page: False for page in pages}\n",
        "                for term, pages in self.index_entries.items()\n",
        "            }\n",
        "\n",
        "            # First pass: Try to find matches\n",
        "            for term, target_pages in self.index_entries.items():\n",
        "                for page_num in target_pages:\n",
        "                    page_content = self.get_page_content(page_num)\n",
        "                    match = self.find_term_in_text(term, page_content)\n",
        "\n",
        "                    if match and self.add_index_at_match(term, match, page_num):\n",
        "                        self.added_terms_tracking[term][page_num] = True\n",
        "\n",
        "            # Second pass: Add remaining terms to page centers\n",
        "            for term, page_status in self.added_terms_tracking.items():\n",
        "                for page_num, added in page_status.items():\n",
        "                    if not added:\n",
        "                        if self.add_index_to_page(page_num, term, self.modified_content):\n",
        "                            self.added_terms_tracking[term][page_num] = True\n",
        "\n",
        "            # Verify and report\n",
        "            self.verify_additions()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def verify_additions(self):\n",
        "        \"\"\"Verify all terms were added and report status.\"\"\"\n",
        "        total_required = sum(len(pages) for pages in self.index_entries.values())\n",
        "        total_added = sum(\n",
        "            sum(1 for added in page_status.values() if added)\n",
        "            for page_status in self.added_terms_tracking.values()\n",
        "        )\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nVerification results:\")\n",
        "            print(f\"Required additions: {total_required}\")\n",
        "            print(f\"Successful additions: {total_added}\")\n",
        "\n",
        "            if total_added < total_required:\n",
        "                print(\"\\nMissing terms:\")\n",
        "                for term, page_status in self.added_terms_tracking.items():\n",
        "                    missing_pages = [\n",
        "                        page for page, added in page_status.items()\n",
        "                        if not added\n",
        "                    ]\n",
        "                    if missing_pages:\n",
        "                        print(f\"- '{term}' not added to pages: {missing_pages}\")\n",
        "\n",
        "    def estimate_page_boundaries(self, content):\n",
        "        \"\"\"Estimate page boundaries based on content length.\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        total_lines = len(lines)\n",
        "        avg_lines_per_page = 45\n",
        "\n",
        "        current_line = 0\n",
        "        current_page = 1\n",
        "\n",
        "        while current_line < total_lines:\n",
        "            end_line = min(current_line + avg_lines_per_page, total_lines)\n",
        "            self.page_boundaries[current_page] = (current_line, end_line)\n",
        "            current_line = end_line + 1\n",
        "            current_page += 1\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Created {len(self.page_boundaries)} page boundaries\")\n",
        "\n",
        "    def add_index_at_match(self, term, match, page_num):\n",
        "        \"\"\"Add index at matched location.\"\"\"\n",
        "        try:\n",
        "            start_line, end_line = self.page_boundaries[page_num]\n",
        "            line_num = start_line\n",
        "            char_count = 0\n",
        "\n",
        "            for i, line in enumerate(self.modified_content[start_line:end_line]):\n",
        "                if char_count <= match.start() < char_count + len(line) + 1:\n",
        "                    line_num = start_line + i\n",
        "                    break\n",
        "                char_count += len(line) + 1\n",
        "\n",
        "            self.modified_content[line_num] = (\n",
        "                self.modified_content[line_num][:match.end() - char_count] +\n",
        "                f'\\\\index{{{term}}}' +\n",
        "                self.modified_content[line_num][match.end() - char_count:]\n",
        "            )\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding index at match for '{term}' on page {page_num}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def add_index_to_page(self, page_num, term, lines):\n",
        "        \"\"\"Add index to middle of page - guaranteed to add if page exists.\"\"\"\n",
        "        if page_num not in self.page_boundaries:\n",
        "            return False\n",
        "\n",
        "        start_line, end_line = self.page_boundaries[page_num]\n",
        "        middle_line = (start_line + end_line) // 2\n",
        "        added = False\n",
        "\n",
        "        # Look around middle\n",
        "        for offset in range(end_line - start_line):\n",
        "            for line_num in [middle_line + offset, middle_line - offset]:\n",
        "                if start_line <= line_num < end_line:\n",
        "                    line = lines[line_num].strip()\n",
        "                    if line and not line.startswith('\\\\'):\n",
        "                        lines[line_num] = f\"{lines[line_num].rstrip()}\\\\index{{{term}}}\"\n",
        "                        added = True\n",
        "                        break\n",
        "            if added:\n",
        "                break\n",
        "\n",
        "        # Fallback: add to first non-empty line\n",
        "        if not added:\n",
        "            for i in range(start_line, end_line):\n",
        "                if lines[i].strip() and not lines[i].strip().startswith('\\\\'):\n",
        "                    lines[i] = f\"{lines[i].rstrip()}\\\\index{{{term}}}\"\n",
        "                    added = True\n",
        "                    break\n",
        "\n",
        "        # Last resort: add to first line of page\n",
        "        if not added:\n",
        "            lines[start_line] = f\"{lines[start_line].rstrip()}\\\\index{{{term}}}\"\n",
        "            added = True\n",
        "\n",
        "        return added\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save modified content to file.\"\"\"\n",
        "        content = '\\n'.join(self.modified_content)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nSaved modified content to: {output_file}\")\n",
        "\n",
        "    def add_index_package(self):\n",
        "        \"\"\"Add required index packages if not present.\"\"\"\n",
        "        preamble_end = -1\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\begin{document}' in line:\n",
        "                preamble_end = i\n",
        "                break\n",
        "\n",
        "        if preamble_end != -1:\n",
        "            preamble = '\\n'.join(self.modified_content[:preamble_end])\n",
        "            packages_to_add = []\n",
        "\n",
        "            if '\\\\usepackage{makeidx}' not in preamble:\n",
        "                packages_to_add.append('\\\\usepackage{makeidx}')\n",
        "            if '\\\\makeindex' not in preamble:\n",
        "                packages_to_add.append('\\\\makeindex')\n",
        "\n",
        "            if packages_to_add:\n",
        "                self.modified_content[preamble_end:preamble_end] = packages_to_add\n",
        "\n",
        "        # Add \\printindex before \\end{document}\n",
        "        doc_end = len(self.modified_content)\n",
        "        for i, line in enumerate(self.modified_content):\n",
        "            if '\\\\end{document}' in line:\n",
        "                doc_end = i\n",
        "                break\n",
        "\n",
        "        if '\\\\printindex' not in '\\n'.join(self.modified_content):\n",
        "            self.modified_content.insert(doc_end, '\\\\printindex')\n",
        "\n",
        "def process_files(tex_file_path, index_file_path, output_file_path=None):\n",
        "    \"\"\"Main function to process files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_indexed.tex')\n",
        "\n",
        "    print(f\"Processing LaTeX file: {tex_file_path}\")\n",
        "    print(f\"Using index file: {index_file_path}\")\n",
        "\n",
        "    with open(index_file_path, 'r', encoding='utf-8') as f:\n",
        "        index_content = f.read()\n",
        "\n",
        "    indexer = LatexIndexer()\n",
        "    indexer.parse_index_list(index_content)\n",
        "    indexer.process_latex_file(tex_file_path)\n",
        "    indexer.add_index_package()\n",
        "    indexer.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKLwjmI9mp5T",
        "outputId": "484ef376-aabc-4e6f-8795-870412685abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing LaTeX file: output-gpt-processed.tex\n",
            "Using index file: sample-indices-2.txt\n",
            "Parsed 65 index terms\n",
            "Created 41 page boundaries\n",
            "\n",
            "Verification results:\n",
            "Required additions: 75\n",
            "Successful additions: 7\n",
            "\n",
            "Missing terms:\n",
            "- 'A/B testing' not added to pages: [86]\n",
            "- 'Aaron Schwartz case' not added to pages: [68]\n",
            "- 'AB testing' not added to pages: [137]\n",
            "- 'academic data' not added to pages: [66]\n",
            "- 'accuracy' not added to pages: [215, 228]\n",
            "- 'activation function' not added to pages: [380]\n",
            "- 'AdaBoost' not added to pages: [364]\n",
            "- 'add-one discounting' not added to pages: [357]\n",
            "- 'agglomerative cluster trees' not added to pages: [338]\n",
            "- 'aggregation mechanisms' not added to pages: [83]\n",
            "- 'Akaike information criterion' not added to pages: [289, 335]\n",
            "- 'algorithm analysis' not added to pages: [397]\n",
            "- 'Amazon Turk' not added to pages: [67, 84]\n",
            "- 'tasks assigned' not added to pages: [85]\n",
            "- 'Turkers' not added to pages: [84]\n",
            "- 'American basketball players' not added to pages: [97]\n",
            "- 'analogies' not added to pages: [312]\n",
            "- 'anchoring' not added to pages: [82]\n",
            "- 'angular distance' not added to pages: [310]\n",
            "- 'Anscombe's Quartet' not added to pages: [159]\n",
            "- 'AOL' not added to pages: [64]\n",
            "- 'API' not added to pages: [65]\n",
            "- 'application program interfaces' not added to pages: [65]\n",
            "- 'area under the ROC curve' not added to pages: [219]\n",
            "- 'Aristotle' not added to pages: [326, 327]\n",
            "- 'Arrow's impossibility theorem' not added to pages: [84, 114]\n",
            "- 'artifacts' not added to pages: [69]\n",
            "- 'Ascombe quartet' not added to pages: [272]\n",
            "- 'associativity' not added to pages: [244]\n",
            "- 'autocorrelation' not added to pages: [46]\n",
            "- 'average link' not added to pages: [340]\n",
            "- 'Babbage, Charles' not added to pages: [57, 90]\n",
            "- 'backpropagation' not added to pages: [382]\n",
            "- 'bagging' not added to pages: [362]\n",
            "- 'balanced training classes' not added to pages: [295]\n",
            "- 'bar charts' not added to pages: [179]\n",
            "- 'best practices' not added to pages: [181]\n",
            "- 'stacked' not added to pages: [181]\n",
            "- 'baseline models' not added to pages: [210]\n",
            "- 'for classification' not added to pages: [210]\n",
            "- 'for value prediction' not added to pages: [212]\n",
            "- 'Bayes' theorem' not added to pages: [150, 205, 299]\n",
            "- 'Baysian information criteria' not added to pages: [289]\n",
            "- 'bell-shaped' not added to pages: [123, 141]\n",
            "- 'distribution' not added to pages: [101]\n",
            "- 'bias' not added to pages: [202, 417]\n",
            "- 'lexicographic' not added to pages: [405]\n",
            "- 'numerical' not added to pages: [405]\n",
            "- 'temporal' not added to pages: [405]\n",
            "- 'bias–variance trade-off' not added to pages: [202]\n",
            "- 'big data' not added to pages: [391]\n",
            "- 'algorithms' not added to pages: [397]\n",
            "- 'bad data' not added to pages: [392]\n",
            "- 'statistics' not added to pages: [392]\n",
            "- 'big oh analysis' not added to pages: [397]\n",
            "- 'binary relations' not added to pages: [320]\n",
            "- 'binary search' not added to pages: [398]\n",
            "- 'binomial distribution' not added to pages: [123]\n",
            "\n",
            "Saved modified content to: output-gpt-processed_indexed.tex\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'output-gpt-processed_indexed.tex'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_files('output-gpt-processed.tex', 'sample-indices-2.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qdQG7DIi07U"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "class LatexBibliographyMapper:\n",
        "    def __init__(self, debug=True):\n",
        "        self.bib_entries = {}\n",
        "        self.modified_content = []\n",
        "        self.debug = debug\n",
        "        self.citations_found = set()\n",
        "\n",
        "    def parse_bib_list(self, bib_text):\n",
        "        \"\"\"Parse the bibliography list into a dictionary of citations and their full references.\"\"\"\n",
        "        # Split entries if they're separated by blank lines\n",
        "        entries = re.split(r'\\n\\s*\\n', bib_text)\n",
        "\n",
        "        for entry in entries:\n",
        "            entry = entry.strip()\n",
        "            if not entry:\n",
        "                continue\n",
        "\n",
        "            # Try to match [key] format at the start of the line\n",
        "            match = re.match(r'\\[(.*?)\\](.*)', entry)\n",
        "            if match:\n",
        "                key, reference = match.groups()\n",
        "                key = key.strip()\n",
        "                reference = reference.strip()\n",
        "                self.bib_entries[key] = reference\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"Parsed {len(self.bib_entries)} bibliography entries\")\n",
        "            print(\"Bibliography keys:\", list(self.bib_entries.keys()))\n",
        "\n",
        "    def process_latex_file(self, input_file):\n",
        "        \"\"\"Read and process the LaTeX file.\"\"\"\n",
        "        try:\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"File length: {len(content)} characters\")\n",
        "                print(\"Contains \\\\begin{document}:\", '\\\\begin{document}' in content)\n",
        "\n",
        "            # Normalize line endings\n",
        "            content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "\n",
        "            # Find document structure\n",
        "            doc_begin = content.find('\\\\begin{document}')\n",
        "            doc_end = content.find('\\\\end{document}')\n",
        "\n",
        "            if doc_begin == -1:\n",
        "                raise ValueError(\"Could not find \\\\begin{document}\")\n",
        "\n",
        "            if doc_end == -1:\n",
        "                raise ValueError(\"Could not find \\\\end{document}\")\n",
        "\n",
        "            # Split into parts\n",
        "            preamble = content[:doc_begin]\n",
        "            body = content[doc_begin:doc_end]\n",
        "            ending = content[doc_end:]\n",
        "\n",
        "            # Add required packages to preamble if not present\n",
        "            if '\\\\usepackage{cite}' not in preamble:\n",
        "                preamble = preamble.rstrip() + '\\n\\\\usepackage{cite}\\n'\n",
        "\n",
        "            # Process citations in the body text\n",
        "            # Look for citations in [key] format\n",
        "            citations = re.finditer(r'\\[([^\\]]+)\\]', body)\n",
        "\n",
        "            modified_body = body\n",
        "            for citation in citations:\n",
        "                cite_key = citation.group(1).strip()\n",
        "                if cite_key in self.bib_entries:\n",
        "                    self.citations_found.add(cite_key)\n",
        "                    # Replace [key] with \\cite{key} if it's not already a \\cite command\n",
        "                    if not re.match(r'\\\\cite{.*}', citation.group(0)):\n",
        "                        modified_body = modified_body.replace(\n",
        "                            citation.group(0),\n",
        "                            f'\\\\cite{{{cite_key}}}'\n",
        "                        )\n",
        "\n",
        "            # Create bibliography section\n",
        "            bib_section = '\\n\\\\begin{thebibliography}{99}\\n'\n",
        "\n",
        "            # Add only the cited references\n",
        "            for i, key in enumerate(sorted(self.citations_found), 1):\n",
        "                if key in self.bib_entries:\n",
        "                    bib_section += f'\\\\bibitem{{{key}}} {self.bib_entries[key]}\\n'\n",
        "\n",
        "            bib_section += '\\\\end{thebibliography}\\n'\n",
        "\n",
        "            # Add bibliography section before \\end{document}\n",
        "            if '\\\\begin{thebibliography}' not in content:\n",
        "                ending = bib_section + ending\n",
        "\n",
        "            # Combine everything back\n",
        "            final_content = preamble + modified_body + ending\n",
        "\n",
        "            # Save the modified content\n",
        "            self.modified_content = final_content.split('\\n')\n",
        "\n",
        "            if self.debug:\n",
        "                print(f\"Found and processed {len(self.citations_found)} citations\")\n",
        "                print(\"Citations found:\", sorted(list(self.citations_found)))\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            print(\"Trying with different encodings...\")\n",
        "            encodings = ['latin-1', 'utf-16', 'cp1252']\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    with open(input_file, 'r', encoding=encoding) as f:\n",
        "                        content = f.read()\n",
        "                    print(f\"Successfully read file with {encoding} encoding\")\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "            else:\n",
        "                raise ValueError(f\"Could not read file with any of these encodings: utf-8, {', '.join(encodings)}\")\n",
        "\n",
        "    def save_modified_file(self, output_file):\n",
        "        \"\"\"Save the modified content to a new file.\"\"\"\n",
        "        content = '\\n'.join(self.modified_content)\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"\\nSaved modified content to {output_file}\")\n",
        "            print(\"\\nTo generate the bibliography, run:\")\n",
        "            print(f\"1. pdflatex {output_file}\")\n",
        "            print(f\"2. pdflatex {output_file}\")  # Second run to resolve citations\n",
        "\n",
        "def process_bibliography(tex_file_path, bib_file_path, output_file_path=None):\n",
        "    \"\"\"Main function to process the files.\"\"\"\n",
        "    if output_file_path is None:\n",
        "        output_file_path = tex_file_path.replace('.tex', '_with_bib.tex')\n",
        "\n",
        "    print(f\"Processing LaTeX file: {tex_file_path}\")\n",
        "    print(f\"Using bibliography file: {bib_file_path}\")\n",
        "    print(f\"Output will be saved to: {output_file_path}\")\n",
        "\n",
        "    with open(bib_file_path, 'r', encoding='utf-8') as f:\n",
        "        bib_content = f.read()\n",
        "\n",
        "    mapper = LatexBibliographyMapper()\n",
        "    mapper.parse_bib_list(bib_content)\n",
        "    mapper.process_latex_file(tex_file_path)\n",
        "    mapper.save_modified_file(output_file_path)\n",
        "\n",
        "    return output_file_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "def parse_citation(citation: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extract the tag and full citation from a bibliography entry.\n",
        "\n",
        "    Args:\n",
        "        citation (str): Raw citation text\n",
        "    Returns:\n",
        "        Tuple[str, str]: (tag if present, full citation text)\n",
        "    \"\"\"\n",
        "    match = re.match(r'\\[(.*?)\\]\\s*&\\s*(.*?)\\\\\\\\', citation.strip())\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, citation.strip()\n",
        "\n",
        "def generate_tag(citation: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate standardized bibliography tag from citation text.\n",
        "\n",
        "    Args:\n",
        "        citation (str): Full citation text\n",
        "    Returns:\n",
        "        str: Generated tag following the format:\n",
        "             - Single author: First3Letters + Year2Digits\n",
        "             - 2-3 authors: AuthorInitials + Year2Digits\n",
        "             - >3 authors: FirstAuthor3Letters + Next2AuthorInitials + '+' + Year2Digits\n",
        "    \"\"\"\n",
        "    # Extract year (19XX or 20XX)\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', citation)\n",
        "    year = year_match.group(0) if year_match else ''\n",
        "\n",
        "    # Get authors (text before first period or \\textit)\n",
        "    first_part = citation.split('.')[0]\n",
        "    first_part = first_part.split(r'\\textit')[0]\n",
        "\n",
        "    # Extract author names (Capitalized words)\n",
        "    authors = re.findall(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]*)*)', first_part)\n",
        "\n",
        "    if not authors:\n",
        "        return f\"Unknown{year}\"\n",
        "\n",
        "    first_author = authors[0].split()[-1]  # Last name of first author\n",
        "\n",
        "    # Generate tag based on number of authors\n",
        "    if len(authors) == 1:\n",
        "        return f\"{first_author[:3]}{year[-2:]}\"\n",
        "    elif len(authors) <= 3:\n",
        "        # Use initials of all authors\n",
        "        initials = ''.join(author.split()[-1][0] for author in authors)\n",
        "        return f\"{initials}{year[-2:]}\"\n",
        "    else:\n",
        "        # First author + next 2 author initials + '+' + year\n",
        "        return f\"{first_author[:3]}{''.join(a.split()[-1][0] for a in authors[1:3])}+{year[-2:]}\"\n",
        "\n",
        "def bipartite_match_hungarian(generated_tags: List[str], actual_tags: List[str],\n",
        "                            similarity_threshold: float = 0.3) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Match generated tags to actual tags using Hungarian Algorithm.\n",
        "\n",
        "    Args:\n",
        "        generated_tags (List[str]): List of generated citation tags\n",
        "        actual_tags (List[str]): List of actual citation tags\n",
        "        similarity_threshold (float): Minimum similarity score to consider a match\n",
        "    Returns:\n",
        "        Dict[str, str]: Mapping of generated tags to best matching actual tags\n",
        "    \"\"\"\n",
        "    cost_matrix = np.zeros((len(generated_tags), len(actual_tags)))\n",
        "\n",
        "    # Build cost matrix\n",
        "    for i, gen_tag in enumerate(generated_tags):\n",
        "        for j, act_tag in enumerate(actual_tags):\n",
        "            # Calculate base similarity\n",
        "            base_score = SequenceMatcher(None, gen_tag.lower(), act_tag.lower()).ratio()\n",
        "\n",
        "            # Add bonus for matching years\n",
        "            year_bonus = 0.2 if (\n",
        "                re.search(r'\\d{2}', gen_tag) and\n",
        "                re.search(r'\\d{2}', act_tag) and\n",
        "                re.search(r'\\d{2}', gen_tag).group(0) == re.search(r'\\d{2}', act_tag).group(0)\n",
        "            ) else 0\n",
        "\n",
        "            # Convert similarity to cost (negative for maximization)\n",
        "            cost_matrix[i][j] = -(base_score + year_bonus)\n",
        "\n",
        "    # Apply Hungarian algorithm\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    # Create matches dictionary with threshold\n",
        "    matches = {}\n",
        "    for i, j in zip(row_ind, col_ind):\n",
        "        if -cost_matrix[i][j] > similarity_threshold:\n",
        "            matches[generated_tags[i]] = actual_tags[j]\n",
        "\n",
        "    return matches\n",
        "\n",
        "def process_bibliography(file_path: str) -> Dict[str, Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Process bibliography file and return matches with full citations.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to bibliography file\n",
        "    Returns:\n",
        "        Dict with two keys:\n",
        "            'matches': Dict mapping generated tags to actual tags\n",
        "            'citations': Dict mapping generated tags to full citations\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        citations = f.readlines()\n",
        "\n",
        "    actual_tags = []\n",
        "    generated_tags = []\n",
        "    citations_dict = {}\n",
        "\n",
        "    # Process each citation\n",
        "    for citation in citations:\n",
        "        if not citation.strip():\n",
        "            continue\n",
        "\n",
        "        actual_tag, full_citation = parse_citation(citation)\n",
        "        if actual_tag:\n",
        "            actual_tags.append(actual_tag)\n",
        "\n",
        "        generated_tag = generate_tag(full_citation)\n",
        "        generated_tags.append(generated_tag)\n",
        "        citations_dict[generated_tag] = full_citation\n",
        "\n",
        "    # Find optimal matches\n",
        "    matches = bipartite_match_hungarian(generated_tags, actual_tags)\n",
        "\n",
        "    return {\n",
        "        'matches': matches,\n",
        "        'citations': citations_dict\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the bibliography matching system.\"\"\"\n",
        "    file_path = \"bib.txt\"\n",
        "    result = process_bibliography(file_path)\n",
        "\n",
        "    print(\"\\nMatched Citations:\")\n",
        "    print(\"-\" * 80)\n",
        "    for gen_tag, act_tag in result['matches'].items():\n",
        "        similarity = SequenceMatcher(None, gen_tag.lower(), act_tag.lower()).ratio()\n",
        "        print(f\"\\nGenerated Tag: {gen_tag}\")\n",
        "        print(f\"Actual Tag: {act_tag}\")\n",
        "        print(f\"Similarity Score: {similarity:.3f}\")\n",
        "        print(f\"Citation: {result['citations'][gen_tag]}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUnHRQN3lHGA",
        "outputId": "d8ecb389-3631-4273-9cc2-f765662cbf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matched Citations:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Sam06\n",
            "Actual Tag: Sam06\n",
            "Similarity Score: 1.000\n",
            "Citation: Hanan Samet. \\textit{Foundations of Multidimensional and Metric Data Structures}. Morgan Kaufmann, 2006. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: SazAS+97\n",
            "Actual Tag: SAMS97\n",
            "Similarity Score: 0.714\n",
            "Citation: George N Sazaklis, Esther M Arkin, Joseph SB Mitchell, and Steven S Skiena. Geometric decision trees for optical character recognition. In \\textit{Proceedings of the 13th Annual Symposium on Computational Geometry}, pages 394–396. ACM, 1997. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: M12\n",
            "Actual Tag: SF12\n",
            "Similarity Score: 0.571\n",
            "Citation: Gail M. Sullivan and Richard Feinn. Using effect size: or why the p-value is not enough. \\textit{J. Graduate Medical Education}, 4:279282, 2012. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Sil12\n",
            "Actual Tag: Sil12\n",
            "Similarity Score: 1.000\n",
            "Citation: Nate Silver. \\textit{The Signal and the Noise: Why so many predictions fail-but some don’t}. Penguin, 2012. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Six\n",
            "Actual Tag: Ski08\n",
            "Similarity Score: 0.500\n",
            "Citation: % Sixteenth row of bibliography\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Ski12\n",
            "Actual Tag: Ski12\n",
            "Similarity Score: 1.000\n",
            "Citation: Steven Skiena. Redesigning viral genomes. \\textit{Computer}, 45(3):0047–53, 2012. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: SteMB+99\n",
            "Actual Tag: SMB+99\n",
            "Similarity Score: 0.857\n",
            "Citation: Arthur G Stephenson, Daniel R Mulville, Frank H Bauer, Greg A Dukeman, Peter Norvig, Lia S LaPiana, Peter J Rutledge, David Folta, and Robert Sackheim. Mars climate orbiter mishap investigation board phase i report. NASA, Washington, DC, page 44, 1999. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: SanRS+14\n",
            "Actual Tag: SRS+14\n",
            "Similarity Score: 0.857\n",
            "Citation: Paolo Santi, Giovanni Resta, Michael Szell, Stanislav Sobolevsky, Steven H Strogatz, and Carlo Ratti. Quantifying the benefits of vehicle pooling with shareability networks. \\textit{Proceedings of the National Academy of Sciences}, 111(37):13290–13294, 2014. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: SS14\n",
            "Actual Tag: SS15\n",
            "Similarity Score: 0.750\n",
            "Citation: Oleksii Starov and Steven Skiena. GIS technology supports taxi tip prediction. \\textit{Esri Map Book, 2014 User Conference}, July 14-17, San Diego, 2015. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Str11\n",
            "Actual Tag: Str11\n",
            "Similarity Score: 1.000\n",
            "Citation: Gilbert Strang. \\textit{Introduction to Linear Algebra}. Wellesley-Cambridge Press, 2011. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Sur05\n",
            "Actual Tag: Sur05\n",
            "Similarity Score: 1.000\n",
            "Citation: James Surowiecki. \\textit{The wisdom of crowds}. Anchor, 2005. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: S13\n",
            "Actual Tag: SW13\n",
            "Similarity Score: 0.857\n",
            "Citation: Steven S. Skiena and Charles B. Ward. \\textit{Who’s Bigger?: Where Historical Figures Really Rank}. Cambridge University Press, 2013. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Tij12\n",
            "Actual Tag: Tij12\n",
            "Similarity Score: 1.000\n",
            "Citation: Henk Tijms. \\textit{Understanding Probability}. Cambridge University Press, 2012. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Tuc88\n",
            "Actual Tag: Tuc88\n",
            "Similarity Score: 1.000\n",
            "Citation: Alan Tucker. \\textit{A Unified Introduction to Linear Algebra: Models, methods, and theory}. Macmillan, 1988. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Tuf83\n",
            "Actual Tag: Tuf83\n",
            "Similarity Score: 1.000\n",
            "Citation: Edward R Tufte. \\textit{The Visual Display of Quantitative Information}. Graphics Press, Cheshire, CT, 1983. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Tuf90\n",
            "Actual Tag: Tuf90\n",
            "Similarity Score: 1.000\n",
            "Citation: Edward R Tufte. \\textit{Envisioning Information}. Graphics Press, Cheshire, CT, 1990. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: Tuf97\n",
            "Actual Tag: Tuf97\n",
            "Similarity Score: 1.000\n",
            "Citation: Edward R Tufte. \\textit{Visual Explanations}. Graphics Press, Cheshire, CT, 1997. \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Generated Tag: AhnMM+08\n",
            "Actual Tag: VAMM+08\n",
            "Similarity Score: 0.800\n",
            "Citation: Luis Von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel Blum. recaptcha: Human-based character recognition via web security measures. \\textit{Science}, 321(5895):1465–1468, 2008. \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm scholarly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oluIcicm0ecn",
        "outputId": "f176986e-e293-4041-b1a8-4a19ff0e2051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting arrow (from scholarly)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.3)\n",
            "Collecting bibtexparser (from scholarly)\n",
            "  Downloading bibtexparser-1.4.2.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from scholarly) (1.2.15)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.3.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from scholarly) (0.27.2)\n",
            "Collecting python-dotenv (from scholarly)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from scholarly) (2.32.3)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from scholarly) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->scholarly) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly)\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->scholarly) (2.6)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from bibtexparser->scholarly) (3.2.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scholarly) (1.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from free-proxy->scholarly) (5.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->scholarly) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (2.2.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio~=0.17 (from selenium->scholarly)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->scholarly) (1.8.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (8.1.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.4)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (24.2)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->scholarly) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->scholarly)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.2)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.2-py3-none-any.whl size=43562 sha256=443ca6930df91355fadf30241330c2f63ef3cd193773cfc2d4b6c3b632997d22\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/71/a1/ace26bfc971a86c092f8932dd34c6dcf965f8c2cc29da7a7c8\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.3-py3-none-any.whl size=6096 sha256=ba4a794c5da5cdb598804c7048ea1e352d5d527aec3c9f145f935abf84cf7aab\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/45/0e/36fc27d383f76ec4e6f876c6584102b5ab6146ae535735a1ea\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: sortedcontainers, fake-useragent, wsproto, types-python-dateutil, python-dotenv, outcome, bibtexparser, trio, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed arrow-1.3.0 bibtexparser-1.4.2 fake-useragent-1.5.1 free-proxy-1.1.3 outcome-1.3.0.post0 python-dotenv-1.0.1 scholarly-1.7.11 selenium-4.27.1 sortedcontainers-2.4.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 trio-0.27.0 trio-websocket-0.11.1 types-python-dateutil-2.9.0.20241003 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import time\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from difflib import SequenceMatcher\n",
        "from scholarly import scholarly\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "2REAJOMN0vVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "from scholarly import scholarly\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ScholarCitationFetcher:\n",
        "    \"\"\"\n",
        "    Handle Google Scholar queries using only title-based search.\n",
        "    \"\"\"\n",
        "    def __init__(self, delay: float = 2.0):\n",
        "        self.delay = delay\n",
        "\n",
        "    def extract_title(self, citation: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the title from citation by taking the longest segment between periods.\n",
        "\n",
        "        Args:\n",
        "            citation (str): Raw citation text\n",
        "        Returns:\n",
        "            str: Extracted title\n",
        "        \"\"\"\n",
        "        # Clean the citation by removing LaTeX commands\n",
        "        cleaned = citation.replace(r'\\textit{', '').replace('}', '')\n",
        "\n",
        "        # Split by periods and get all segments\n",
        "        segments = [seg.strip() for seg in cleaned.split('.')]\n",
        "\n",
        "        # Get the longest segment as the title\n",
        "        if segments:\n",
        "            title = max(segments, key=len)\n",
        "            print(f\"\\nExtracted title: {title}\")\n",
        "            return title\n",
        "        return \"\"\n",
        "\n",
        "    def search_citation(self, citation: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Search Google Scholar using only the extracted title.\n",
        "\n",
        "        Args:\n",
        "            citation (str): Citation text to search for\n",
        "        Returns:\n",
        "            Dict: Enhanced citation data if found, None otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Extract title\n",
        "            title = self.extract_title(citation)\n",
        "            if not title:\n",
        "                print(\"Could not extract title from citation\")\n",
        "                return None\n",
        "\n",
        "            print(f\"Searching for title: {title}\")\n",
        "\n",
        "            # Search using the title\n",
        "            search_result = next(scholarly.search_pubs(title), None)\n",
        "\n",
        "            if search_result:\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                result_data = {\n",
        "                    'title': search_result.get('title', ''),\n",
        "                    'authors': search_result.get('author', []),\n",
        "                    'year': search_result.get('year', ''),\n",
        "                    'citations': search_result.get('num_citations', 0),\n",
        "                    'url': search_result.get('url_scholarbib', ''),\n",
        "                    'venues': search_result.get('venue', ''),\n",
        "                    'abstract': search_result.get('abstract', '')\n",
        "                }\n",
        "\n",
        "                print(\"\\nFound match in Google Scholar:\")\n",
        "                print(f\"Title: {result_data['title']}\")\n",
        "                print(f\"Authors: {', '.join(result_data['authors'])}\")\n",
        "                print(f\"Year: {result_data['year']}\")\n",
        "\n",
        "                return result_data\n",
        "\n",
        "            print(\"No match found in Google Scholar\")\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in search_citation: {e}\")\n",
        "            return None\n",
        "\n",
        "def parse_citation(citation: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extract the tag and full citation from a bibliography entry.\n",
        "\n",
        "    Args:\n",
        "        citation (str): Raw citation text\n",
        "    Returns:\n",
        "        Tuple[str, str]: (tag if present, full citation text)\n",
        "    \"\"\"\n",
        "    match = re.match(r'\\[(.*?)\\]\\s*&\\s*(.*?)\\\\\\\\', citation.strip())\n",
        "    if match:\n",
        "        return match.group(1), match.group(2)\n",
        "    return None, citation.strip()\n",
        "\n",
        "def bipartite_match_hungarian(generated_tags: List[str], actual_tags: List[str],\n",
        "                            similarity_threshold: float = 0.3) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Match generated tags to actual tags using Hungarian Algorithm.\n",
        "\n",
        "    Args:\n",
        "        generated_tags (List[str]): List of generated citation tags\n",
        "        actual_tags (List[str]): List of actual citation tags\n",
        "        similarity_threshold (float): Minimum similarity score to consider a match\n",
        "    Returns:\n",
        "        Dict[str, str]: Mapping of generated tags to best matching actual tags\n",
        "    \"\"\"\n",
        "    if not actual_tags or not generated_tags:\n",
        "        return {}\n",
        "\n",
        "    cost_matrix = np.zeros((len(generated_tags), len(actual_tags)))\n",
        "\n",
        "    # Build cost matrix\n",
        "    for i, gen_tag in enumerate(generated_tags):\n",
        "        for j, act_tag in enumerate(actual_tags):\n",
        "            # Calculate base similarity\n",
        "            base_score = SequenceMatcher(None, gen_tag.lower(), act_tag.lower()).ratio()\n",
        "\n",
        "            # Add bonus for matching years\n",
        "            year_bonus = 0.2 if (\n",
        "                re.search(r'\\d{2}', gen_tag) and\n",
        "                re.search(r'\\d{2}', act_tag) and\n",
        "                re.search(r'\\d{2}', gen_tag).group(0) == re.search(r'\\d{2}', act_tag).group(0)\n",
        "            ) else 0\n",
        "\n",
        "            # Convert similarity to cost (negative for maximization)\n",
        "            cost_matrix[i][j] = -(base_score + year_bonus)\n",
        "\n",
        "    # Apply Hungarian algorithm\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "    # Create matches dictionary with threshold\n",
        "    matches = {}\n",
        "    for i, j in zip(row_ind, col_ind):\n",
        "        if -cost_matrix[i][j] > similarity_threshold:\n",
        "            matches[generated_tags[i]] = actual_tags[j]\n",
        "\n",
        "    return matches\n",
        "\n",
        "def generate_tag_from_scholar(scholar_data: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Generate a standardized bibliography tag using Google Scholar data.\n",
        "\n",
        "    Args:\n",
        "        scholar_data (Dict): Scholar citation data containing authors, year, etc.\n",
        "    Returns:\n",
        "        str: Generated tag following the format:\n",
        "             - Single author: First3Letters + Year2Digits\n",
        "             - 2-3 authors: AuthorInitials + Year2Digits\n",
        "             - >3 authors: FirstAuthor3Letters + Next2AuthorInitials + '+' + Year2Digits\n",
        "    \"\"\"\n",
        "    if not scholar_data:\n",
        "        return None\n",
        "\n",
        "    authors = scholar_data.get('authors', [])\n",
        "    year = scholar_data.get('year', '')\n",
        "\n",
        "    if not authors or not year:\n",
        "        return None\n",
        "\n",
        "    # Extract last names (assume last word in author name is last name)\n",
        "    last_names = [author.split()[-1] for author in authors]\n",
        "\n",
        "    # Clean up year to get last 2 digits\n",
        "    year_suffix = str(year)[-2:]\n",
        "\n",
        "    if len(authors) == 1:\n",
        "        # Single author: First three letters of last name + year\n",
        "        return f\"{last_names[0][:3]}{year_suffix}\"\n",
        "\n",
        "    elif len(authors) <= 3:\n",
        "        # 2-3 authors: Initial of each author's last name + year\n",
        "        initials = ''.join(name[0] for name in last_names)\n",
        "        return f\"{initials}{year_suffix}\"\n",
        "\n",
        "    else:\n",
        "        # >3 authors: First author's first 3 letters + next 2 author initials + '+' + year\n",
        "        return f\"{last_names[0][:3]}{last_names[1][0]}{last_names[2][0]}+{year_suffix}\"\n",
        "\n",
        "def generate_tag(citation: str, scholar_data: Dict = None) -> str:\n",
        "    \"\"\"\n",
        "    Generate bibliography tag using Scholar data if available, falling back to regex parsing.\n",
        "\n",
        "    Args:\n",
        "        citation (str): Full citation text\n",
        "        scholar_data (Dict): Optional Google Scholar data\n",
        "    Returns:\n",
        "        str: Generated tag\n",
        "    \"\"\"\n",
        "    # Try generating tag from Scholar data first\n",
        "    if scholar_data:\n",
        "        scholar_tag = generate_tag_from_scholar(scholar_data)\n",
        "        if scholar_tag:\n",
        "            return scholar_tag\n",
        "\n",
        "    # Fallback to regex-based parsing if Scholar data is unavailable or incomplete\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', citation)\n",
        "    year = year_match.group(0) if year_match else ''\n",
        "\n",
        "    first_part = citation.split('.')[0]\n",
        "    first_part = first_part.split(r'\\textit')[0]\n",
        "\n",
        "    authors = re.findall(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]*)*)', first_part)\n",
        "\n",
        "    if not authors:\n",
        "        return f\"Unknown{year[-2:]}\"\n",
        "\n",
        "    first_author = authors[0].split()[-1]\n",
        "\n",
        "    if len(authors) == 1:\n",
        "        return f\"{first_author[:3]}{year[-2:]}\"\n",
        "    elif len(authors) <= 3:\n",
        "        initials = ''.join(author.split()[-1][0] for author in authors)\n",
        "        return f\"{initials}{year[-2:]}\"\n",
        "    else:\n",
        "        return f\"{first_author[:3]}{''.join(a.split()[-1][0] for a in authors[1:3])}+{year[-2:]}\"\n",
        "\n",
        "def process_bibliography(file_path: str, fetch_scholar: bool = True) -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Process bibliography file using enhanced tag generation.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to bibliography file\n",
        "        fetch_scholar (bool): Whether to fetch additional data from Google Scholar\n",
        "    Returns:\n",
        "        Dict containing matches, citations, and enhanced data\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        citations = f.readlines()\n",
        "\n",
        "    actual_tags = []\n",
        "    generated_tags = []\n",
        "    citations_dict = {}\n",
        "    scholar_data = {}\n",
        "\n",
        "    # First pass: Fetch Scholar data if requested\n",
        "    if fetch_scholar:\n",
        "        fetcher = ScholarCitationFetcher()\n",
        "        print(\"\\nFetching citation data from Google Scholar...\")\n",
        "\n",
        "        for citation in tqdm(citations):\n",
        "            if not citation.strip():\n",
        "                continue\n",
        "\n",
        "            actual_tag, full_citation = parse_citation(citation)\n",
        "            scholar_result = fetcher.search_citation(full_citation)\n",
        "\n",
        "            if scholar_result:\n",
        "                # Generate tag using Scholar data\n",
        "                generated_tag = generate_tag(full_citation, scholar_result)\n",
        "                scholar_data[generated_tag] = scholar_result\n",
        "            else:\n",
        "                # Fallback to regex-based tag generation\n",
        "                generated_tag = generate_tag(full_citation)\n",
        "\n",
        "            if actual_tag:\n",
        "                actual_tags.append(actual_tag)\n",
        "            generated_tags.append(generated_tag)\n",
        "            citations_dict[generated_tag] = full_citation\n",
        "\n",
        "    else:\n",
        "        # Process without Scholar data\n",
        "        for citation in citations:\n",
        "            if not citation.strip():\n",
        "                continue\n",
        "\n",
        "            actual_tag, full_citation = parse_citation(citation)\n",
        "            generated_tag = generate_tag(full_citation)\n",
        "\n",
        "            if actual_tag:\n",
        "                actual_tags.append(actual_tag)\n",
        "            generated_tags.append(generated_tag)\n",
        "            citations_dict[generated_tag] = full_citation\n",
        "\n",
        "    # Find optimal matches\n",
        "    matches = bipartite_match_hungarian(generated_tags, actual_tags)\n",
        "\n",
        "    return {\n",
        "        'matches': matches,\n",
        "        'citations': citations_dict,\n",
        "        'scholar_data': scholar_data\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function demonstrating the enhanced tag generation system.\"\"\"\n",
        "    file_path = \"bib.txt\"\n",
        "    result = process_bibliography(file_path, fetch_scholar=True)\n",
        "\n",
        "    print(\"\\nMatched Citations with Enhanced Tags:\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    for gen_tag, act_tag in result['matches'].items():\n",
        "        similarity = SequenceMatcher(None, gen_tag.lower(), act_tag.lower()).ratio()\n",
        "        scholar_info = result['scholar_data'].get(gen_tag, {})\n",
        "\n",
        "        print(f\"\\nGenerated Tag: {gen_tag}\")\n",
        "        print(f\"Actual Tag: {act_tag}\")\n",
        "        print(f\"Similarity Score: {similarity:.3f}\")\n",
        "        print(f\"Citation: {result['citations'][gen_tag]}\")\n",
        "\n",
        "        if scholar_info:\n",
        "            print(\"\\nTag generated using Google Scholar data:\")\n",
        "            print(f\"Authors: {', '.join(scholar_info['authors'])}\")\n",
        "            print(f\"Year: {scholar_info['year']}\")\n",
        "            print(f\"Citations: {scholar_info['citations']}\")\n",
        "        else:\n",
        "            print(\"\\nTag generated using regex parsing (Scholar data unavailable)\")\n",
        "\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "XOXbC7fa0NkV",
        "outputId": "a0b3acf4-872f-4da0-b668-e70255fc3275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching citation data from Google Scholar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/55 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted title: Foundations of Multidimensional and Metric Data Structures\n",
            "Searching for title: Foundations of Multidimensional and Metric Data Structures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d623f2e8c40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b52430f8-e874-4a10-bffc-05e855e47f2e\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d623f2e8a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b52430f8-e874-4a10-bffc-05e855e47f2e\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d623f2e8820>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b52430f8-e874-4a10-bffc-05e855e47f2e\n",
            "  2%|▏         | 1/55 [00:33<29:52, 33.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in search_citation: Cannot Fetch from Google Scholar.\n",
            "\n",
            "Extracted title: % Second row of bibliography\n",
            "Searching for title: % Second row of bibliography\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 3/55 [01:03<17:20, 20.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in search_citation: Cannot Fetch from Google Scholar.\n",
            "\n",
            "Extracted title: In Proceedings of the 13th Annual Symposium on Computational Geometry, pages 394–396\n",
            "Searching for title: In Proceedings of the 13th Annual Symposium on Computational Geometry, pages 394–396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 4/55 [01:36<20:38, 24.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in search_citation: Cannot Fetch from Google Scholar.\n",
            "\n",
            "Extracted title: % Third row of bibliography\n",
            "Searching for title: % Third row of bibliography\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 6/55 [02:07<16:17, 19.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in search_citation: Cannot Fetch from Google Scholar.\n",
            "\n",
            "Extracted title: Using effect size: or why the p-value is not enough\n",
            "Searching for title: Using effect size: or why the p-value is not enough\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 6/55 [02:23<19:31, 23.91s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-172ebc50a54b>\u001b[0m in \u001b[0;36m<cell line: 316>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-172ebc50a54b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;34m\"\"\"Main function demonstrating the enhanced tag generation system.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bib.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_bibliography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_scholar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMatched Citations with Enhanced Tags:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-172ebc50a54b>\u001b[0m in \u001b[0;36mprocess_bibliography\u001b[0;34m(file_path, fetch_scholar)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mactual_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_citation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_citation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mscholar_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_citation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_citation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscholar_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-172ebc50a54b>\u001b[0m in \u001b[0;36msearch_citation\u001b[0;34m(self, citation)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Search using the title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msearch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholarly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_pubs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msearch_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_scholarly.py\u001b[0m in \u001b[0;36msearch_pubs\u001b[0;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                   \u001b[0mcitations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcitations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_low\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myear_low\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear_high\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myear_high\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                                   sort_by=sort_by, include_last_year=include_last_year, start_index=start_index)\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__nav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_publications\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch_citedby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublication_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_navigator.py\u001b[0m in \u001b[0;36msearch_publications\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0m_SearchScholarIterator\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SearchScholarIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch_author_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msortby\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"citedby\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublication_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAuthor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/publication_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nav, url)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pubtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPublicationSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUBLICATION_SEARCH_SNIPPET\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"/scholar?\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mPublicationSource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOURNAL_CITATION_LIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_total_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPublicationParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/publication_parser.py\u001b[0m in \u001b[0;36m_load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# this is temporary until setup json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gs_r gs_or gs_scl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gsc_mpat_ttl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_navigator.py\u001b[0m in \u001b[0;36m_get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;34m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://scholar.google.com{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'\\xa0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_navigator.py\u001b[0m in \u001b[0;36m_get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mtries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_proxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 self.logger.info(\"No other secondary connections possible. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36mget_next_proxy\u001b[0;34m(self, num_tries, old_timeout, old_proxy)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36m_new_session\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;31m# self._session = httpx.Client()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgot_403\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scholarly/_proxy_generator.py\u001b[0m in \u001b[0;36m_close_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_webdriver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_webdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not close webdriver cleanly: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/firefox/webdriver.py\u001b[0m in \u001b[0;36mquit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;34m\"\"\"Closes the browser and shuts down the GeckoDriver executable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# We don't care about the message because something probably has gone wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mquit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \"\"\"\n\u001b[1;32m    505\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mtrimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_large_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrimmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_url_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             return self.request_encode_url(\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/_request_methods.py\u001b[0m in \u001b[0;36mrequest_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"?\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     def request_encode_body(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiBymc6-1FWI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}